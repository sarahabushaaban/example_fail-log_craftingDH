## Readings

Previously, when completing the assigned readings, I would initially finish reading with scattered annotations, and I would proceed to write my blog post **after** I was done with everything. My annotations highlighted what I found most interesting, however, I would have a well-rounded understanding of the purpose, argument, and methods of the article when I had read all of it in its entirety, and my annotations were not reflective of that progress; so by the time I would go to write my blog post, I would have a hard time making sense of those scattered annotations. So, to combat that obstacle, I assigned myself the task of writing my notes as I read, utilizing my annotations - which I planned to improve, and give more substance - but also by noting condensed versions of the facts that I felt presented the key themes of the article, as I read through it. 

I caught myself being guided by my classmates annotations, which I can understand is concurrent with the collaborative course environment, but I also found myself unable to make my own impressions without being influenced by the existing annotations. To ensure I was guided by my own impressions, I disabled the Hypothesis.is highlights, *temporarily*, and I found that my eyes no longer focused on what was already highlighted. I would then make my annotations, and I feel like that made my annotations a more accurate representation of my original thoughts. I did have the annotation tab open though, and I was able to see my classmates’ annotations update as I scrolled down, so after reading I would scroll through those annotations to experience their views and respond whenever I felt like I had something to offer. 



[Building A Volunteer Community: Results and Findings from Transcribe Bentham](http://www.digitalhumanities.org/dhq/vol/6/2/000125/000125.html) details the construction of a crowdsourced initiative, a transcription project by the name of  ‘Transcribe Bentham’ with the aim of digitizing Bentham’s writings. The article outlines the processes of recruitment, advertising, community, and the progression of the project. 

Super-transcribers being “the highly-motivated backbone” of the project was really a projection to me of how much work can be accomplished by those who are dedicated to the task of preserving and digitizing history. 70% of transcriptions were done by them… That statistic shocked me far more than the realization that 49% of publicity was achieved through online media. I wish there were more specific statistics about where those super-transcribers heard about the project initially, to understand if maybe a pattern can be recognized to understand where the most helpful, or contributive members to digital humanities can be reached. I think there’s a great value in the digital historian community on Twitter, which I have become aware of after following Dr. Graham’s account and discovering how these educated professionals exchange their ideas and work together and help each other find the resources that they need, and I was truly inspired by the community’s constructive attitude. It seemed very welcoming, and I wonder if contemporary projects like this have utilized the platform to reach that community which seems to be truly motivated, those who I believe have the potential to be super-transcribers. 

This reading made me ponder the future of digital transcription projects… Bentham probably only wrote them by hand because that was the only way he could, it was the norm, not even the typewriter was invented yet. But in more recent years, there has been a disciplinary shift to digital. Now volunteers have the task of transcribing his work and digitizing it. But also, is it not possible that contemporary historians are using digital resources to research and write their works? So, is there going to be any need for transcription projects like this in the future? Or maybe it is more accurate to ask, is there going to be a need for these transcription projects, when analyzing work that was conducted *after this shift to digital*?


## Exercises

For the first exercise, the database search was fascinating in that it yielded so many results with such open-ended search restrictions. Things I imagined would yield no results, actually surprised me - for example, searching Saudi Arabia has a place of commemoration for WWII soldiers produced 1 result. But I’m sure I could conduct an entire study surrounding that one result. Although I didn’t find any results for my surname, exploring the database with different search filters was fun. To think about searches that yielded thousands of results, and the possibilities for what could be understood with all of that data, it is pretty exciting. 

But exercise 2 was the real issue; I didn’t read the whole first step and so I just followed the link, and tried to follow Ian’s instructions, not skipping ahead to step 2 like we were instructed. So I thought I needed to install Xcode. But to download Xcode, I needed a software update. Reminder to self: stop clicking “remind me tomorrow” when an update is suggested. It was kind of a blessing in disguise though - because when I restarted my laptop after the update, I went back to the workbook to find the link, and this time I read it all - skip to step 2 - you don’t need Xcode, you have DHBox. Another reminder to self: read through instructions thoroughly before attempting an exercise. Following Milligan’s instructions thereafter was a little simpler, although I detailed my realizations more closely in my fail log. 
The next step was downloading the 14th Canadian General Hospital war diaries, but in order to create that directory, to exit back to my parent directory, I learned I simply input $ cd ~ ; this takes me back to my parent directory. Copying and pasting the next code, I included the $ and didn’t realize, and that’s why it didn’t initially work for me. So I dictated it without copying and pasting, and have decided that seems like a far better learning strategy anyway. 
From thereon out, the rest of the steps were VERY straightforward and I found myself becoming more fluent with the language of commands. 

The third exercise concerned utilizing Text Coding Initiative. The first step was vetting the website. At first glance it looked credible to me because of all of the data that was available, categorized by date, and seemingly organized - to a degree. But as I was navigating the website, I decided to search the collection for a term I’m certain must be mentioned to a large degree in documents discussing the European trade slave - Morocco. A number of results appeared, but there was no real way of efficiently navigating that data without a lot of manual input; also, the documents originally appear as images, but when picking the option to have them transcribed to text, they make a lot less sense, and things get *lost in translation*. I think that took away from its ‘trustworthiness’, for me, but I thought with some work, the data could be presented much more efficiently… Luckily for me that was the next part of the exercise! The page I chose to transcribe was the 61st page, and I tested Zotero with my source, to begin familiarizing myself with the program. When I completed the transcription, and completed the exercise prompts where applicable, I spent a lot of time trying to figure out what went wrong with my code because it didn’t show up when I would drag it onto Safari. I then gave in and installed Firefox and it finally worked… a eureka moment? Not really - I just felt like I wasted a lot of my energy. 

The fourth exercise concerned getting familiar with APIs, which is a little difficult for me and I’m not sure I entirely understand it. For what I do understand, it involves a process of communicating a task to the machine that involves the culmination of a list of relevant search results on the user’s instruction, which is in the form of JSON (machine-readable, not human-readable). 

The sixth exercise involved first involved installing Tesseract and Imagemagick; I had no issues installing Tesseract but when I would input sudo apt-get install imagemagick some files would not download, and I started getting very flustered about why the command didn’t work. I then searched imagemagick on Slack and found that some of my classmates had been encountering a similar issue and had found a resolution to the issue, by inputing the command sudo-apt get update, and afterwards I was able to install imagemagick software and the required packages by following the steps in the workbook. 
