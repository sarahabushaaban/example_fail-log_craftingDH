<h1>Module 4: Fail Log</h1>

<h2>Exercise 1: Network Visualization</h2>

* I began to do exercise 1, but then I realized that I had some issues with my dataset from the previous exercise. Some of my data was scattered, something I didn’t realize when I was doing exercise 1 in module 3. So I thought it might be best to leave it for last, so that I could go back to the data and fix those inconsistencies. 

<h2>Exercise 2: Topic Modeling Tool</h2>
 
* I decided to download Dr. Graham’s copy of the Colonial Newspaper Database. The process was a little tricky, because it was a csv file, and the instructions on the workbook were more clear about the war-diary-text file that we had. But after watching and re-watching the tutorial videos, I managed to implement the same instructions on the csv file. I realized soon after I would have to unzip the zip file, which I hadn’t thought to do earlier, and that’s why the TopicModelingTool application wouldn’t let me upload the csv file as input.
* I placed the csv file in the war-diary-text folder. I then zipped the folder on DHBox using this command: $ zip -r wardiaryfiles.zip war-diary-text, and unzipped the folder. This then allowed me to upload the folder as input in the TopicModelingTool.
* 10 topics were generated, although they didn’t make much sense. They were just words, and they didn’t necessarily go together or make sense on their own.
* I clicked on each of the topics, but the text I found within them was the same, over and over again; fragmented versions of the CND.CSV file. I think this was to do with the fact that it was one csv file. 

<h2>Exercise 3: Topic Modeling in Studio</h2>

* I opened RStudio on DHBox, and created a new project in a new directory.
* I then clicked on the green plus sign, and selected “new r script” and input this code as instructed in the workbook:    
    * install.packages("mallet")
    * library("mallet")
    * install.packages("RCurl")
    * library("RCurl")
* I ran each of those lines without any issues. Everything loaded correctly.
* I input this command: x <- getURL("https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv", .opts = list(ssl.verifypeer = FALSE)) ; which uses a special package to grab data and put it in a variable that is easier to analyze. “x” refers to the variable. The URL is Dr. Graham’s Colonial Newspaper Database copy. 
* I input this command: documents <- read.csv(text = x, col.names=c("Article_ID", "Newspaper Title", "Newspaper City", "Newspaper Province", "Newspaper Country", "Year", "Month", "Day", "Article Type", "Text", "Keywords"), colClasses=rep("character", 3), sep=",", quote="") ; which creates a new variable, “documents” & inputs the “read.csv” command, which takes all the data in the variable “x” and clarifies the columns in the “documents” variable. (e.g. Newspaper Title, Newspaper City, etc.)
* To take the information from one particular column, like the “Year” column, I would change the variable to “documents$Year” for example.
    * As directed in the workbook, I input this command: counts <- table(documents$Newspaper.City) which instructs RStudio to create a new variable, “counts” and put information from the “Newspaper City” column in the “documents” variable. (The . between Newspaper and City is because there is a space)
* To create a simple bar chart, I input this command, as directed: barplot(counts, main="Cities", xlab="Number of Articles”)… and wow! All this code, that seemingly seems like a lot to digest, has produced something very easily digestible!!
* Input these commands, which created a variable “years” and puts information from the “Year” column in the “documents” variable, like with “counts” and “Newspaper City” variables previously.
    * years <- table(documents$Year) barplot(years, main="Publication Year", xlab="Year", ylab="Number of Articles")
    * This created another bar chart.
* I experimented further with this feature, choosing the variable to be months because I thought it might be interesting to see if there were any patterns to be found throughout the months of the year. I did this by first inputting these commands:
    * months <- table(documents$Month) barplot(months, main=“Publication Month”, xlab=“Month”, ylab=“Number of Articles”)
    * And it didn’t work. I’m kind of glad that it didn’t because it really forced me to keep trying everything, I realized my error was a missing space key. This command was what finally worked for me.
    * barplot(months, main = "Publication Month", xlab="Month", ylab = "Number of Articles")
    * What this revealed to me was there was an influx of articles in July; I wonder why that is!
* I then copied the text in the link, provided by Matt Jockers, to create en.txt, and run the following mallet command:
    * mallet.instances <- mallet.import(documents$Article_ID, documents$Text, "en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
    * This takes the article ID and text to the mallet routine, and “token.regexp” cleans the newspaper articles by applying a regular expression against them. 
* I input the command documents <- mallet.read.dir("/home/sarahabushaaban/war-diary-text") to pull up the text and read it for keywords. 
* I then did as directed in the workbook, and input these commands:
    * #set the number of desired topics num.topics <- 20 ; this is self-explanatory.
    * topic.model <- MalletLDA(num.topics); this creates a variable “topic.model” to be filled by Mallet.
* I then input these commands which are also self-explanatory.
    * topic.model$loadDocuments(mallet.instances)  Get the vocabulary, and some statistics about word frequencies.  These may be useful in further curating the stopword list. vocabulary <-topic.model$getVocabulary() word.freqs <- mallet.word.freqs(topic.model) head(word.freqs)
    * This produced a table that pulled the major terms an their frequencies.
    * I then input this command write.csv(word.freqs, "word-freqs.csv" ), which creates a csv file of this data. 
* I then input this very long list of commands, which forms the topic model.
    * Optimize hyperparameters every 20 iterations,
    * after 50 burn-in iterations. topic.model$setAlphaOptimization(20, 50)  Now train a model. Note that hyperparameter optimization is on, by default.  We can specify the number of iterations. Here we'll use a large-ish round number.  When you run the next line, a *lot* of information will scroll through your console.  Just be patient and wait til it hits that 1000 iteration. topic.model$train(1000)  Run through a few iterations where we pick the best topic for each token, rather than sampling from the posterior distribution. topic.model$maximize(10) Get the probability of topics in documents and the probability of words in topics.  By default, these functions return raw word counts. Here we want probabilities,  so we normalize, and add "smoothing" so that nothing has exactly 0 probability. doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T) topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
* I then followed the rest of the instructions, which was fairly simple.
* Then, I faced an interesting issue while trying to create a histogram. It kept saying “error: margins too large”, and I realized that I had condensed the window displaying the histogram. Even after enlarging it, the data looked very clustered, so I clicked zoom to get a clearer look at it. Here is a picture that I saved. 

<h2>Exercise 4: Text Analysis With Overview</h2>

* I created an account with Overview. I uploaded the CSV file from earlier, however, when I clicked upload nothing would happen. When I switched browsers from Mozilla to Chrome, the page finally loaded. I guess with these sorts of work, it’s clear, some browsers are better with programs than others. 
* I searched the term “government”, and 110 mentions in the documents were listed. 
* I found it really interesting the way the search could be refined, to search for “government” for example in “keywords”. It reminded me of the ways we were taught to filter through the Summon system on the Carleton library website, in a much more precise manner.

For the next exercises, I feel really depleted, and plan to revisit them soon.
